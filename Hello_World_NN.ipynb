{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTdrBISY5ElfJqkgYrRf7Z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorflow Version\n",
        "\n",
        "## Hello World: Linear regression"
      ],
      "metadata": {
        "id": "59N-8iicciX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3shvXbximBaH",
        "outputId": "27d4e19e-5834-4405-f033-79ed6db9c3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import Sequential, layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data that serves as input.\n",
        "\n",
        "xs = np.array([-1, 0, 1, 2, 3, 4], dtype='float32')\n",
        "ys = np.array([-3, -1, 1, 3, 5, 7], dtype='float32')"
      ],
      "metadata": {
        "id": "1sLJA2ljmRDz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation and compilation.\n",
        "\n",
        "model = Sequential([\n",
        "                    layers.Dense(\n",
        "                        input_shape=[1],  # Because we are going to take one xs value at a time in the NN.\n",
        "                        units=1           # One neuron.\n",
        "                    )\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='mse',\n",
        "    optimizer='sgd',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "Zccejd2Vmz_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf17f49-66ab-4555-cf85-76000e125e4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "An9EbhTW2PGZ",
        "outputId": "d5d0acc8-6c07-42b8-cf14-20e116bbe22b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m2\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2\u001b[0m (8.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (8.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2\u001b[0m (8.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (8.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the training data in the model.\n",
        "\n",
        "model.fit(\n",
        "    xs,\n",
        "    ys,\n",
        "    epochs=500\n",
        ")"
      ],
      "metadata": {
        "id": "cQlac9jsnUdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308ac37f-d8da-4846-99fe-60de37a66031"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 21.0240\n",
            "Epoch 2/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0000e+00 - loss: 16.8121\n",
            "Epoch 3/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0000e+00 - loss: 13.4927\n",
            "Epoch 4/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.0000e+00 - loss: 10.8758\n",
            "Epoch 5/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 8.8115\n",
            "Epoch 6/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1667 - loss: 7.1822\n",
            "Epoch 7/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.1667 - loss: 5.8953\n",
            "Epoch 8/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.1667 - loss: 4.8777\n",
            "Epoch 9/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1667 - loss: 4.0722\n",
            "Epoch 10/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 3.4337\n",
            "Epoch 11/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1667 - loss: 2.9266\n",
            "Epoch 12/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.1667 - loss: 2.5230\n",
            "Epoch 13/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.1667 - loss: 2.2010\n",
            "Epoch 14/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.1667 - loss: 1.9432\n",
            "Epoch 15/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.1667 - loss: 1.7360\n",
            "Epoch 16/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.1667 - loss: 1.5687\n",
            "Epoch 17/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 1.4330\n",
            "Epoch 18/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 1.3221\n",
            "Epoch 19/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1667 - loss: 1.2308\n",
            "Epoch 20/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1667 - loss: 1.1551\n",
            "Epoch 21/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1667 - loss: 1.0918\n",
            "Epoch 22/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1667 - loss: 1.0381\n",
            "Epoch 23/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1667 - loss: 0.9922\n",
            "Epoch 24/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.9525\n",
            "Epoch 25/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1667 - loss: 0.9178\n",
            "Epoch 26/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 0.8870\n",
            "Epoch 27/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.1667 - loss: 0.8593\n",
            "Epoch 28/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.1667 - loss: 0.8343\n",
            "Epoch 29/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.1667 - loss: 0.8113\n",
            "Epoch 30/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.1667 - loss: 0.7901\n",
            "Epoch 31/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.1667 - loss: 0.7703\n",
            "Epoch 32/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.7516\n",
            "Epoch 33/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.1667 - loss: 0.7339\n",
            "Epoch 34/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.1667 - loss: 0.7171\n",
            "Epoch 35/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.1667 - loss: 0.7010\n",
            "Epoch 36/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.6855\n",
            "Epoch 37/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.1667 - loss: 0.6706\n",
            "Epoch 38/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.1667 - loss: 0.6561\n",
            "Epoch 39/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.1667 - loss: 0.6421\n",
            "Epoch 40/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.1667 - loss: 0.6285\n",
            "Epoch 41/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1667 - loss: 0.6153\n",
            "Epoch 42/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1667 - loss: 0.6024\n",
            "Epoch 43/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 0.5898\n",
            "Epoch 44/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 0.5775\n",
            "Epoch 45/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.5655\n",
            "Epoch 46/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1667 - loss: 0.5538\n",
            "Epoch 47/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.1667 - loss: 0.5424\n",
            "Epoch 48/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.5312\n",
            "Epoch 49/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.1667 - loss: 0.5202\n",
            "Epoch 50/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.1667 - loss: 0.5095\n",
            "Epoch 51/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.1667 - loss: 0.4990\n",
            "Epoch 52/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.1667 - loss: 0.4887\n",
            "Epoch 53/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.1667 - loss: 0.4787\n",
            "Epoch 54/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.1667 - loss: 0.4688\n",
            "Epoch 55/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.1667 - loss: 0.4592\n",
            "Epoch 56/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.1667 - loss: 0.4497\n",
            "Epoch 57/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.4405\n",
            "Epoch 58/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1667 - loss: 0.4314\n",
            "Epoch 59/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 0.4226\n",
            "Epoch 60/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.4139\n",
            "Epoch 61/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 0.4054\n",
            "Epoch 62/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.3971\n",
            "Epoch 63/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.3889\n",
            "Epoch 64/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.3809\n",
            "Epoch 65/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.3731\n",
            "Epoch 66/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.3654\n",
            "Epoch 67/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.3579\n",
            "Epoch 68/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.3506\n",
            "Epoch 69/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.3434\n",
            "Epoch 70/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.3363\n",
            "Epoch 71/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.3294\n",
            "Epoch 72/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.3226\n",
            "Epoch 73/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.3160\n",
            "Epoch 74/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.3095\n",
            "Epoch 75/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.3032\n",
            "Epoch 76/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.2969\n",
            "Epoch 77/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.2908\n",
            "Epoch 78/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 0.2849\n",
            "Epoch 79/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 0.2790\n",
            "Epoch 80/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 0.2733\n",
            "Epoch 81/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 0.2677\n",
            "Epoch 82/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.2622\n",
            "Epoch 83/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.2568\n",
            "Epoch 84/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1667 - loss: 0.2515\n",
            "Epoch 85/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.2463\n",
            "Epoch 86/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.1667 - loss: 0.2413\n",
            "Epoch 87/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 0.2363\n",
            "Epoch 88/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.2315\n",
            "Epoch 89/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.2267\n",
            "Epoch 90/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.2221\n",
            "Epoch 91/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.2175\n",
            "Epoch 92/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.2130\n",
            "Epoch 93/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 0.2086\n",
            "Epoch 94/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.2044\n",
            "Epoch 95/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.2002\n",
            "Epoch 96/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 0.1961\n",
            "Epoch 97/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1667 - loss: 0.1920\n",
            "Epoch 98/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1667 - loss: 0.1881\n",
            "Epoch 99/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.1842\n",
            "Epoch 100/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.1804\n",
            "Epoch 101/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.1767\n",
            "Epoch 102/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.1731\n",
            "Epoch 103/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.1695\n",
            "Epoch 104/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1667 - loss: 0.1661\n",
            "Epoch 105/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.1626\n",
            "Epoch 106/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.1593\n",
            "Epoch 107/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.1560\n",
            "Epoch 108/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.1528\n",
            "Epoch 109/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.1497\n",
            "Epoch 110/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.1466\n",
            "Epoch 111/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 0.1436\n",
            "Epoch 112/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.1407\n",
            "Epoch 113/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.1378\n",
            "Epoch 114/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 0.1349\n",
            "Epoch 115/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1667 - loss: 0.1322\n",
            "Epoch 116/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.1295\n",
            "Epoch 117/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1667 - loss: 0.1268\n",
            "Epoch 118/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.1242\n",
            "Epoch 119/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.1216\n",
            "Epoch 120/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.1191\n",
            "Epoch 121/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.1167\n",
            "Epoch 122/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.1143\n",
            "Epoch 123/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.1119\n",
            "Epoch 124/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.1096\n",
            "Epoch 125/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.1074\n",
            "Epoch 126/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.1052\n",
            "Epoch 127/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.1030\n",
            "Epoch 128/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.1009\n",
            "Epoch 129/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 0.0988\n",
            "Epoch 130/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.0968\n",
            "Epoch 131/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0948\n",
            "Epoch 132/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 0.0929\n",
            "Epoch 133/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0910\n",
            "Epoch 134/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 0.0891\n",
            "Epoch 135/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0873\n",
            "Epoch 136/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0855\n",
            "Epoch 137/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1667 - loss: 0.0837\n",
            "Epoch 138/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0820\n",
            "Epoch 139/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0803\n",
            "Epoch 140/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0787\n",
            "Epoch 141/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0770\n",
            "Epoch 142/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0755\n",
            "Epoch 143/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0739\n",
            "Epoch 144/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0724\n",
            "Epoch 145/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0709\n",
            "Epoch 146/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0695\n",
            "Epoch 147/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0680\n",
            "Epoch 148/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0666\n",
            "Epoch 149/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.0653\n",
            "Epoch 150/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0639\n",
            "Epoch 151/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1667 - loss: 0.0626\n",
            "Epoch 152/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 0.0613\n",
            "Epoch 153/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.0601\n",
            "Epoch 154/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0588\n",
            "Epoch 155/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0576\n",
            "Epoch 156/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.1667 - loss: 0.0564\n",
            "Epoch 157/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 0.0553\n",
            "Epoch 158/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0541\n",
            "Epoch 159/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0530\n",
            "Epoch 160/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0519\n",
            "Epoch 161/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0509\n",
            "Epoch 162/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0498\n",
            "Epoch 163/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0488\n",
            "Epoch 164/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0478\n",
            "Epoch 165/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 0.0468\n",
            "Epoch 166/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0459\n",
            "Epoch 167/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0449\n",
            "Epoch 168/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0440\n",
            "Epoch 169/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0431\n",
            "Epoch 170/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.0422\n",
            "Epoch 171/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0413\n",
            "Epoch 172/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0405\n",
            "Epoch 173/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0397\n",
            "Epoch 174/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 0.0388\n",
            "Epoch 175/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0380\n",
            "Epoch 176/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0373\n",
            "Epoch 177/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0365\n",
            "Epoch 178/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0357\n",
            "Epoch 179/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0350\n",
            "Epoch 180/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0343\n",
            "Epoch 181/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0336\n",
            "Epoch 182/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0329\n",
            "Epoch 183/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0322\n",
            "Epoch 184/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0316\n",
            "Epoch 185/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1667 - loss: 0.0309\n",
            "Epoch 186/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.0303\n",
            "Epoch 187/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 0.0297\n",
            "Epoch 188/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0290\n",
            "Epoch 189/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.0285\n",
            "Epoch 190/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.0279\n",
            "Epoch 191/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.0273\n",
            "Epoch 192/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0267\n",
            "Epoch 193/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0262\n",
            "Epoch 194/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0256\n",
            "Epoch 195/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 0.0251\n",
            "Epoch 196/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 0.0246\n",
            "Epoch 197/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0241\n",
            "Epoch 198/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.1667 - loss: 0.0236\n",
            "Epoch 199/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.0231\n",
            "Epoch 200/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0226\n",
            "Epoch 201/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0222\n",
            "Epoch 202/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0217\n",
            "Epoch 203/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0213\n",
            "Epoch 204/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0208\n",
            "Epoch 205/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0204\n",
            "Epoch 206/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0200\n",
            "Epoch 207/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0196\n",
            "Epoch 208/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0192\n",
            "Epoch 209/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.0188\n",
            "Epoch 210/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 0.0184\n",
            "Epoch 211/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0180\n",
            "Epoch 212/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0177\n",
            "Epoch 213/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.0173\n",
            "Epoch 214/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 0.0169\n",
            "Epoch 215/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 0.0166\n",
            "Epoch 216/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0162\n",
            "Epoch 217/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0159\n",
            "Epoch 218/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0156\n",
            "Epoch 219/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0153\n",
            "Epoch 220/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0150\n",
            "Epoch 221/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0146\n",
            "Epoch 222/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0143\n",
            "Epoch 223/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0140\n",
            "Epoch 224/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.0138\n",
            "Epoch 225/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0135\n",
            "Epoch 226/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0132\n",
            "Epoch 227/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0129\n",
            "Epoch 228/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 0.0127\n",
            "Epoch 229/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0124\n",
            "Epoch 230/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0121\n",
            "Epoch 231/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0119\n",
            "Epoch 232/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 0.0117\n",
            "Epoch 233/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0114\n",
            "Epoch 234/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0112\n",
            "Epoch 235/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 0.0110\n",
            "Epoch 236/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0107\n",
            "Epoch 237/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0105\n",
            "Epoch 238/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0103\n",
            "Epoch 239/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 0.0101\n",
            "Epoch 240/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0099\n",
            "Epoch 241/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0097\n",
            "Epoch 242/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.0095\n",
            "Epoch 243/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0093\n",
            "Epoch 244/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0091\n",
            "Epoch 245/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0089\n",
            "Epoch 246/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0087\n",
            "Epoch 247/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.0085\n",
            "Epoch 248/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.1667 - loss: 0.0084\n",
            "Epoch 249/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 0.0082\n",
            "Epoch 250/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.0080\n",
            "Epoch 251/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.0079\n",
            "Epoch 252/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 0.0077\n",
            "Epoch 253/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0075\n",
            "Epoch 254/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0074\n",
            "Epoch 255/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0072\n",
            "Epoch 256/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0071\n",
            "Epoch 257/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0069\n",
            "Epoch 258/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 0.0068\n",
            "Epoch 259/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 0.0067\n",
            "Epoch 260/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0065\n",
            "Epoch 261/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0064\n",
            "Epoch 262/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 0.0063\n",
            "Epoch 263/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0061\n",
            "Epoch 264/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0060\n",
            "Epoch 265/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0059\n",
            "Epoch 266/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0058\n",
            "Epoch 267/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 0.0056\n",
            "Epoch 268/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0055\n",
            "Epoch 269/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0054\n",
            "Epoch 270/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0053\n",
            "Epoch 271/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.0052\n",
            "Epoch 272/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 0.0051\n",
            "Epoch 273/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.0050\n",
            "Epoch 274/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 0.0049\n",
            "Epoch 275/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.0048\n",
            "Epoch 276/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1667 - loss: 0.0047\n",
            "Epoch 277/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 0.0046\n",
            "Epoch 278/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0045\n",
            "Epoch 279/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0044\n",
            "Epoch 280/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0043\n",
            "Epoch 281/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 0.0042\n",
            "Epoch 282/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0041\n",
            "Epoch 283/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0040\n",
            "Epoch 284/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0040\n",
            "Epoch 285/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.0039\n",
            "Epoch 286/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 0.0038\n",
            "Epoch 287/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 0.0037\n",
            "Epoch 288/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0036\n",
            "Epoch 289/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.0036\n",
            "Epoch 290/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0035\n",
            "Epoch 291/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0034\n",
            "Epoch 292/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0034\n",
            "Epoch 293/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0033\n",
            "Epoch 294/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 0.0032\n",
            "Epoch 295/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.1667 - loss: 0.0032\n",
            "Epoch 296/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.0031\n",
            "Epoch 297/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 0.0030\n",
            "Epoch 298/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0030\n",
            "Epoch 299/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.0029\n",
            "Epoch 300/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0028\n",
            "Epoch 301/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 0.0028\n",
            "Epoch 302/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.0027\n",
            "Epoch 303/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0027\n",
            "Epoch 304/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 0.0026\n",
            "Epoch 305/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0026\n",
            "Epoch 306/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 0.0025\n",
            "Epoch 307/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 0.0025\n",
            "Epoch 308/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0024\n",
            "Epoch 309/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0024\n",
            "Epoch 310/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0023\n",
            "Epoch 311/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.1667 - loss: 0.0023\n",
            "Epoch 312/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 0.0022\n",
            "Epoch 313/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.1667 - loss: 0.0022\n",
            "Epoch 314/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0021\n",
            "Epoch 315/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0021\n",
            "Epoch 316/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 0.0020\n",
            "Epoch 317/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0020\n",
            "Epoch 318/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.1667 - loss: 0.0020\n",
            "Epoch 319/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0019\n",
            "Epoch 320/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0019\n",
            "Epoch 321/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0018\n",
            "Epoch 322/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.1667 - loss: 0.0018\n",
            "Epoch 323/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 0.0018\n",
            "Epoch 324/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0017\n",
            "Epoch 325/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.1667 - loss: 0.0017\n",
            "Epoch 326/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0017\n",
            "Epoch 327/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 0.0016\n",
            "Epoch 328/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 0.0016\n",
            "Epoch 329/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 0.0016\n",
            "Epoch 330/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 0.0015\n",
            "Epoch 331/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 0.0015\n",
            "Epoch 332/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 0.0015\n",
            "Epoch 333/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0014\n",
            "Epoch 334/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0014\n",
            "Epoch 335/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1667 - loss: 0.0014\n",
            "Epoch 336/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.1667 - loss: 0.0013\n",
            "Epoch 337/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 0.0013\n",
            "Epoch 338/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.0013\n",
            "Epoch 339/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0013\n",
            "Epoch 340/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 0.0012\n",
            "Epoch 341/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0012\n",
            "Epoch 342/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0012\n",
            "Epoch 343/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0012\n",
            "Epoch 344/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 0.0011\n",
            "Epoch 345/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 0.0011\n",
            "Epoch 346/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 0.0011\n",
            "Epoch 347/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 0.0011\n",
            "Epoch 348/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.1667 - loss: 0.0010\n",
            "Epoch 349/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 0.0010\n",
            "Epoch 350/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 0.0010\n",
            "Epoch 351/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 9.8608e-04\n",
            "Epoch 352/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 9.6582e-04\n",
            "Epoch 353/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 9.4599e-04\n",
            "Epoch 354/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 9.2656e-04\n",
            "Epoch 355/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 9.0752e-04\n",
            "Epoch 356/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 8.8888e-04\n",
            "Epoch 357/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 8.7063e-04\n",
            "Epoch 358/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 8.5274e-04\n",
            "Epoch 359/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.1667 - loss: 8.3523e-04\n",
            "Epoch 360/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 8.1807e-04\n",
            "Epoch 361/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 8.0127e-04\n",
            "Epoch 362/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 7.8481e-04\n",
            "Epoch 363/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 7.6869e-04\n",
            "Epoch 364/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 7.5290e-04\n",
            "Epoch 365/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 7.3744e-04\n",
            "Epoch 366/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 7.2229e-04\n",
            "Epoch 367/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 7.0746e-04\n",
            "Epoch 368/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 6.9293e-04\n",
            "Epoch 369/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 6.7869e-04\n",
            "Epoch 370/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 6.6475e-04\n",
            "Epoch 371/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.1667 - loss: 6.5110e-04\n",
            "Epoch 372/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1667 - loss: 6.3773e-04\n",
            "Epoch 373/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 6.2463e-04\n",
            "Epoch 374/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 6.1180e-04\n",
            "Epoch 375/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 5.9923e-04\n",
            "Epoch 376/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 5.8692e-04\n",
            "Epoch 377/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 5.7487e-04\n",
            "Epoch 378/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 5.6306e-04\n",
            "Epoch 379/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 5.5150e-04\n",
            "Epoch 380/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 5.4016e-04\n",
            "Epoch 381/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 5.2907e-04\n",
            "Epoch 382/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 5.1820e-04\n",
            "Epoch 383/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 5.0756e-04\n",
            "Epoch 384/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 4.9713e-04\n",
            "Epoch 385/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 4.8692e-04\n",
            "Epoch 386/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 4.7692e-04\n",
            "Epoch 387/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 4.6712e-04\n",
            "Epoch 388/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 4.5753e-04\n",
            "Epoch 389/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 4.4813e-04\n",
            "Epoch 390/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 4.3893e-04\n",
            "Epoch 391/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1667 - loss: 4.2991e-04\n",
            "Epoch 392/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1667 - loss: 4.2108e-04\n",
            "Epoch 393/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 4.1243e-04\n",
            "Epoch 394/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 4.0396e-04\n",
            "Epoch 395/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 3.9566e-04\n",
            "Epoch 396/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 3.8753e-04\n",
            "Epoch 397/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 3.7957e-04\n",
            "Epoch 398/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 3.7178e-04\n",
            "Epoch 399/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 3.6414e-04\n",
            "Epoch 400/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1667 - loss: 3.5666e-04\n",
            "Epoch 401/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 3.4933e-04\n",
            "Epoch 402/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 3.4216e-04\n",
            "Epoch 403/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 3.3513e-04\n",
            "Epoch 404/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.1667 - loss: 3.2825e-04\n",
            "Epoch 405/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 3.2150e-04\n",
            "Epoch 406/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 3.1490e-04\n",
            "Epoch 407/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 3.0843e-04\n",
            "Epoch 408/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 3.0210e-04\n",
            "Epoch 409/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 2.9589e-04\n",
            "Epoch 410/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 2.8981e-04\n",
            "Epoch 411/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 2.8386e-04\n",
            "Epoch 412/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1667 - loss: 2.7803e-04\n",
            "Epoch 413/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 2.7232e-04\n",
            "Epoch 414/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.1667 - loss: 2.6672e-04\n",
            "Epoch 415/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 2.6124e-04\n",
            "Epoch 416/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 2.5588e-04\n",
            "Epoch 417/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 2.5063e-04\n",
            "Epoch 418/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 2.4548e-04\n",
            "Epoch 419/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 2.4043e-04\n",
            "Epoch 420/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 2.3550e-04\n",
            "Epoch 421/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 2.3066e-04\n",
            "Epoch 422/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 2.2592e-04\n",
            "Epoch 423/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 2.2128e-04\n",
            "Epoch 424/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 2.1673e-04\n",
            "Epoch 425/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 2.1228e-04\n",
            "Epoch 426/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 2.0792e-04\n",
            "Epoch 427/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 2.0365e-04\n",
            "Epoch 428/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 1.9947e-04\n",
            "Epoch 429/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 1.9537e-04\n",
            "Epoch 430/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.1667 - loss: 1.9136e-04\n",
            "Epoch 431/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 1.8742e-04\n",
            "Epoch 432/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 1.8358e-04\n",
            "Epoch 433/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 1.7980e-04\n",
            "Epoch 434/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 1.7611e-04\n",
            "Epoch 435/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 1.7249e-04\n",
            "Epoch 436/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 1.6895e-04\n",
            "Epoch 437/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 1.6548e-04\n",
            "Epoch 438/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 1.6208e-04\n",
            "Epoch 439/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 1.5875e-04\n",
            "Epoch 440/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 1.5549e-04\n",
            "Epoch 441/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 1.5230e-04\n",
            "Epoch 442/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 1.4917e-04\n",
            "Epoch 443/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.1667 - loss: 1.4611e-04\n",
            "Epoch 444/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 1.4310e-04\n",
            "Epoch 445/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 1.4017e-04\n",
            "Epoch 446/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.1667 - loss: 1.3729e-04\n",
            "Epoch 447/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 1.3447e-04\n",
            "Epoch 448/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 1.3170e-04\n",
            "Epoch 449/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 1.2900e-04\n",
            "Epoch 450/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 1.2635e-04\n",
            "Epoch 451/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 1.2375e-04\n",
            "Epoch 452/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 1.2121e-04\n",
            "Epoch 453/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 1.1872e-04\n",
            "Epoch 454/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1667 - loss: 1.1628e-04\n",
            "Epoch 455/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 1.1390e-04\n",
            "Epoch 456/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 1.1155e-04\n",
            "Epoch 457/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 1.0926e-04\n",
            "Epoch 458/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 1.0702e-04\n",
            "Epoch 459/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 1.0482e-04\n",
            "Epoch 460/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.1667 - loss: 1.0267e-04\n",
            "Epoch 461/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 1.0056e-04\n",
            "Epoch 462/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 9.8493e-05\n",
            "Epoch 463/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 9.6470e-05\n",
            "Epoch 464/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 9.4489e-05\n",
            "Epoch 465/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 9.2547e-05\n",
            "Epoch 466/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 9.0646e-05\n",
            "Epoch 467/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 8.8784e-05\n",
            "Epoch 468/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1667 - loss: 8.6962e-05\n",
            "Epoch 469/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 8.5175e-05\n",
            "Epoch 470/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 8.3425e-05\n",
            "Epoch 471/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 8.1711e-05\n",
            "Epoch 472/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1667 - loss: 8.0033e-05\n",
            "Epoch 473/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 7.8389e-05\n",
            "Epoch 474/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1667 - loss: 7.6778e-05\n",
            "Epoch 475/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 7.5203e-05\n",
            "Epoch 476/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1667 - loss: 7.3658e-05\n",
            "Epoch 477/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.1667 - loss: 7.2145e-05\n",
            "Epoch 478/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 7.0662e-05\n",
            "Epoch 479/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 6.9211e-05\n",
            "Epoch 480/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1667 - loss: 6.7788e-05\n",
            "Epoch 481/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 6.6396e-05\n",
            "Epoch 482/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.1667 - loss: 6.5033e-05\n",
            "Epoch 483/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 6.3696e-05\n",
            "Epoch 484/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.1667 - loss: 6.2389e-05\n",
            "Epoch 485/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1667 - loss: 6.1108e-05\n",
            "Epoch 486/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 5.9852e-05\n",
            "Epoch 487/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1667 - loss: 5.8623e-05\n",
            "Epoch 488/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1667 - loss: 5.7419e-05\n",
            "Epoch 489/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 5.6239e-05\n",
            "Epoch 490/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 5.5085e-05\n",
            "Epoch 491/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.1667 - loss: 5.3953e-05\n",
            "Epoch 492/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.1667 - loss: 5.2844e-05\n",
            "Epoch 493/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1667 - loss: 5.1760e-05\n",
            "Epoch 494/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1667 - loss: 5.0696e-05\n",
            "Epoch 495/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1667 - loss: 4.9655e-05\n",
            "Epoch 496/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1667 - loss: 4.8634e-05\n",
            "Epoch 497/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1667 - loss: 4.7635e-05\n",
            "Epoch 498/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 4.6657e-05\n",
            "Epoch 499/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.1667 - loss: 4.5699e-05\n",
            "Epoch 500/500\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.1667 - loss: 4.4761e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e253f505450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction.\n",
        "\n",
        "model.predict(np.array([10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z46hFi-Kn48-",
        "outputId": "9c6e0c03-6d2f-4457-ab6c-13573add955c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18.98048]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Version\n",
        "\n",
        "## Hello World: Linear Regression"
      ],
      "metadata": {
        "id": "BOEbDvWLcr1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "0gRcs7ytdYZ8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the single linear layer.\n",
        "\n",
        "modelPytorch = torch.nn.Linear(\n",
        "    in_features=1,\n",
        "    out_features=1\n",
        "    )\n",
        "\n",
        "print(modelPytorch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oenFRFUdupI",
        "outputId": "14873796-66c5-4795-baf5-cb95360cce5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=1, out_features=1, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping the tensors. Originally they were (1,6), this time is (6,1).\n",
        "# What this does is give a full row at a time to the model.\n",
        "\n",
        "xsTensor = torch.tensor(xs).reshape(-1,1)\n",
        "ysTensor = torch.tensor(ys).reshape(-1,1)\n",
        "print(xsTensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_TqCWu_3xNm",
        "outputId": "60d4009a-7e1c-46f3-cdb3-46c99591040c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.],\n",
            "        [ 0.],\n",
            "        [ 1.],\n",
            "        [ 2.],\n",
            "        [ 3.],\n",
            "        [ 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The initial weights and biases of the model.\n",
        "for cell in modelPytorch.parameters():\n",
        "  print(cell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS_iVa2v0aeG",
        "outputId": "3cb84f81-b476-4f85-d6c7-456b633d664e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1791]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.6302], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The loss and optimization parameters.\n",
        "\n",
        "crit = torch.nn.MSELoss()\n",
        "opti = torch.optim.SGD(modelPytorch.parameters(), lr=0.01)\n",
        "print(opti.defaults)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGp-K1Syq5bG",
        "outputId": "fb33daa2-bce3-42a7-c89f-083e423782d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A loop on a number of epochs.\n",
        "lossList = []\n",
        "\n",
        "for epoch in range(500):\n",
        "    pred = modelPytorch(xsTensor)        # Extract predictions on the training data\n",
        "    loss = crit(pred, ysTensor)          # Calculate the loss between the predicted and the original output.\n",
        "\n",
        "    # Backpropagation\n",
        "    opti.zero_grad()                     # Resets the gradients of all model parameters to zero.\n",
        "    loss.backward()                      # The Pytorch automatic gradient function, to extract the loss' gradients.\n",
        "    opti.step()\n",
        "\n",
        "    lossList.append(loss.item())                          # Function that updates the model's parameters based on the calculated gradients.\n",
        "\n",
        "    print(f\"Epoch {epoch}:--------->loss: {loss:>7f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-2sZv2MylA2",
        "outputId": "6ea7c84b-a65f-42f7-82a3-836a537b9bb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:--------->loss: 16.534990\n",
            "Epoch 1:--------->loss: 13.499366\n",
            "Epoch 2:--------->loss: 11.101009\n",
            "Epoch 3:--------->loss: 9.204231\n",
            "Epoch 4:--------->loss: 7.702274\n",
            "Epoch 5:--------->loss: 6.511141\n",
            "Epoch 6:--------->loss: 5.564744\n",
            "Epoch 7:--------->loss: 4.811083\n",
            "Epoch 8:--------->loss: 4.209245\n",
            "Epoch 9:--------->loss: 3.727036\n",
            "Epoch 10:--------->loss: 3.339125\n",
            "Epoch 11:--------->loss: 3.025578\n",
            "Epoch 12:--------->loss: 2.770708\n",
            "Epoch 13:--------->loss: 2.562171\n",
            "Epoch 14:--------->loss: 2.390251\n",
            "Epoch 15:--------->loss: 2.247302\n",
            "Epoch 16:--------->loss: 2.127302\n",
            "Epoch 17:--------->loss: 2.025514\n",
            "Epoch 18:--------->loss: 1.938204\n",
            "Epoch 19:--------->loss: 1.862434\n",
            "Epoch 20:--------->loss: 1.795889\n",
            "Epoch 21:--------->loss: 1.736743\n",
            "Epoch 22:--------->loss: 1.683558\n",
            "Epoch 23:--------->loss: 1.635200\n",
            "Epoch 24:--------->loss: 1.590773\n",
            "Epoch 25:--------->loss: 1.549570\n",
            "Epoch 26:--------->loss: 1.511032\n",
            "Epoch 27:--------->loss: 1.474716\n",
            "Epoch 28:--------->loss: 1.440272\n",
            "Epoch 29:--------->loss: 1.407421\n",
            "Epoch 30:--------->loss: 1.375941\n",
            "Epoch 31:--------->loss: 1.345656\n",
            "Epoch 32:--------->loss: 1.316424\n",
            "Epoch 33:--------->loss: 1.288133\n",
            "Epoch 34:--------->loss: 1.260689\n",
            "Epoch 35:--------->loss: 1.234019\n",
            "Epoch 36:--------->loss: 1.208062\n",
            "Epoch 37:--------->loss: 1.182768\n",
            "Epoch 38:--------->loss: 1.158095\n",
            "Epoch 39:--------->loss: 1.134011\n",
            "Epoch 40:--------->loss: 1.110484\n",
            "Epoch 41:--------->loss: 1.087490\n",
            "Epoch 42:--------->loss: 1.065008\n",
            "Epoch 43:--------->loss: 1.043018\n",
            "Epoch 44:--------->loss: 1.021504\n",
            "Epoch 45:--------->loss: 1.000452\n",
            "Epoch 46:--------->loss: 0.979846\n",
            "Epoch 47:--------->loss: 0.959676\n",
            "Epoch 48:--------->loss: 0.939929\n",
            "Epoch 49:--------->loss: 0.920596\n",
            "Epoch 50:--------->loss: 0.901665\n",
            "Epoch 51:--------->loss: 0.883127\n",
            "Epoch 52:--------->loss: 0.864974\n",
            "Epoch 53:--------->loss: 0.847197\n",
            "Epoch 54:--------->loss: 0.829787\n",
            "Epoch 55:--------->loss: 0.812736\n",
            "Epoch 56:--------->loss: 0.796037\n",
            "Epoch 57:--------->loss: 0.779682\n",
            "Epoch 58:--------->loss: 0.763664\n",
            "Epoch 59:--------->loss: 0.747975\n",
            "Epoch 60:--------->loss: 0.732609\n",
            "Epoch 61:--------->loss: 0.717560\n",
            "Epoch 62:--------->loss: 0.702819\n",
            "Epoch 63:--------->loss: 0.688382\n",
            "Epoch 64:--------->loss: 0.674241\n",
            "Epoch 65:--------->loss: 0.660392\n",
            "Epoch 66:--------->loss: 0.646826\n",
            "Epoch 67:--------->loss: 0.633540\n",
            "Epoch 68:--------->loss: 0.620526\n",
            "Epoch 69:--------->loss: 0.607780\n",
            "Epoch 70:--------->loss: 0.595296\n",
            "Epoch 71:--------->loss: 0.583068\n",
            "Epoch 72:--------->loss: 0.571091\n",
            "Epoch 73:--------->loss: 0.559360\n",
            "Epoch 74:--------->loss: 0.547871\n",
            "Epoch 75:--------->loss: 0.536617\n",
            "Epoch 76:--------->loss: 0.525595\n",
            "Epoch 77:--------->loss: 0.514798\n",
            "Epoch 78:--------->loss: 0.504224\n",
            "Epoch 79:--------->loss: 0.493867\n",
            "Epoch 80:--------->loss: 0.483723\n",
            "Epoch 81:--------->loss: 0.473787\n",
            "Epoch 82:--------->loss: 0.464055\n",
            "Epoch 83:--------->loss: 0.454523\n",
            "Epoch 84:--------->loss: 0.445187\n",
            "Epoch 85:--------->loss: 0.436042\n",
            "Epoch 86:--------->loss: 0.427086\n",
            "Epoch 87:--------->loss: 0.418313\n",
            "Epoch 88:--------->loss: 0.409721\n",
            "Epoch 89:--------->loss: 0.401305\n",
            "Epoch 90:--------->loss: 0.393062\n",
            "Epoch 91:--------->loss: 0.384988\n",
            "Epoch 92:--------->loss: 0.377080\n",
            "Epoch 93:--------->loss: 0.369335\n",
            "Epoch 94:--------->loss: 0.361748\n",
            "Epoch 95:--------->loss: 0.354318\n",
            "Epoch 96:--------->loss: 0.347040\n",
            "Epoch 97:--------->loss: 0.339911\n",
            "Epoch 98:--------->loss: 0.332929\n",
            "Epoch 99:--------->loss: 0.326091\n",
            "Epoch 100:--------->loss: 0.319393\n",
            "Epoch 101:--------->loss: 0.312832\n",
            "Epoch 102:--------->loss: 0.306407\n",
            "Epoch 103:--------->loss: 0.300113\n",
            "Epoch 104:--------->loss: 0.293948\n",
            "Epoch 105:--------->loss: 0.287910\n",
            "Epoch 106:--------->loss: 0.281997\n",
            "Epoch 107:--------->loss: 0.276204\n",
            "Epoch 108:--------->loss: 0.270531\n",
            "Epoch 109:--------->loss: 0.264974\n",
            "Epoch 110:--------->loss: 0.259531\n",
            "Epoch 111:--------->loss: 0.254200\n",
            "Epoch 112:--------->loss: 0.248979\n",
            "Epoch 113:--------->loss: 0.243865\n",
            "Epoch 114:--------->loss: 0.238856\n",
            "Epoch 115:--------->loss: 0.233949\n",
            "Epoch 116:--------->loss: 0.229144\n",
            "Epoch 117:--------->loss: 0.224437\n",
            "Epoch 118:--------->loss: 0.219827\n",
            "Epoch 119:--------->loss: 0.215312\n",
            "Epoch 120:--------->loss: 0.210889\n",
            "Epoch 121:--------->loss: 0.206557\n",
            "Epoch 122:--------->loss: 0.202314\n",
            "Epoch 123:--------->loss: 0.198159\n",
            "Epoch 124:--------->loss: 0.194088\n",
            "Epoch 125:--------->loss: 0.190102\n",
            "Epoch 126:--------->loss: 0.186197\n",
            "Epoch 127:--------->loss: 0.182372\n",
            "Epoch 128:--------->loss: 0.178626\n",
            "Epoch 129:--------->loss: 0.174957\n",
            "Epoch 130:--------->loss: 0.171363\n",
            "Epoch 131:--------->loss: 0.167844\n",
            "Epoch 132:--------->loss: 0.164396\n",
            "Epoch 133:--------->loss: 0.161019\n",
            "Epoch 134:--------->loss: 0.157712\n",
            "Epoch 135:--------->loss: 0.154472\n",
            "Epoch 136:--------->loss: 0.151299\n",
            "Epoch 137:--------->loss: 0.148191\n",
            "Epoch 138:--------->loss: 0.145148\n",
            "Epoch 139:--------->loss: 0.142166\n",
            "Epoch 140:--------->loss: 0.139246\n",
            "Epoch 141:--------->loss: 0.136386\n",
            "Epoch 142:--------->loss: 0.133584\n",
            "Epoch 143:--------->loss: 0.130840\n",
            "Epoch 144:--------->loss: 0.128153\n",
            "Epoch 145:--------->loss: 0.125521\n",
            "Epoch 146:--------->loss: 0.122942\n",
            "Epoch 147:--------->loss: 0.120417\n",
            "Epoch 148:--------->loss: 0.117944\n",
            "Epoch 149:--------->loss: 0.115521\n",
            "Epoch 150:--------->loss: 0.113148\n",
            "Epoch 151:--------->loss: 0.110824\n",
            "Epoch 152:--------->loss: 0.108548\n",
            "Epoch 153:--------->loss: 0.106318\n",
            "Epoch 154:--------->loss: 0.104134\n",
            "Epoch 155:--------->loss: 0.101995\n",
            "Epoch 156:--------->loss: 0.099900\n",
            "Epoch 157:--------->loss: 0.097848\n",
            "Epoch 158:--------->loss: 0.095838\n",
            "Epoch 159:--------->loss: 0.093870\n",
            "Epoch 160:--------->loss: 0.091941\n",
            "Epoch 161:--------->loss: 0.090053\n",
            "Epoch 162:--------->loss: 0.088203\n",
            "Epoch 163:--------->loss: 0.086391\n",
            "Epoch 164:--------->loss: 0.084617\n",
            "Epoch 165:--------->loss: 0.082879\n",
            "Epoch 166:--------->loss: 0.081176\n",
            "Epoch 167:--------->loss: 0.079509\n",
            "Epoch 168:--------->loss: 0.077876\n",
            "Epoch 169:--------->loss: 0.076276\n",
            "Epoch 170:--------->loss: 0.074709\n",
            "Epoch 171:--------->loss: 0.073175\n",
            "Epoch 172:--------->loss: 0.071672\n",
            "Epoch 173:--------->loss: 0.070200\n",
            "Epoch 174:--------->loss: 0.068758\n",
            "Epoch 175:--------->loss: 0.067345\n",
            "Epoch 176:--------->loss: 0.065962\n",
            "Epoch 177:--------->loss: 0.064607\n",
            "Epoch 178:--------->loss: 0.063280\n",
            "Epoch 179:--------->loss: 0.061980\n",
            "Epoch 180:--------->loss: 0.060707\n",
            "Epoch 181:--------->loss: 0.059460\n",
            "Epoch 182:--------->loss: 0.058239\n",
            "Epoch 183:--------->loss: 0.057043\n",
            "Epoch 184:--------->loss: 0.055871\n",
            "Epoch 185:--------->loss: 0.054723\n",
            "Epoch 186:--------->loss: 0.053599\n",
            "Epoch 187:--------->loss: 0.052498\n",
            "Epoch 188:--------->loss: 0.051420\n",
            "Epoch 189:--------->loss: 0.050364\n",
            "Epoch 190:--------->loss: 0.049329\n",
            "Epoch 191:--------->loss: 0.048316\n",
            "Epoch 192:--------->loss: 0.047324\n",
            "Epoch 193:--------->loss: 0.046351\n",
            "Epoch 194:--------->loss: 0.045399\n",
            "Epoch 195:--------->loss: 0.044467\n",
            "Epoch 196:--------->loss: 0.043553\n",
            "Epoch 197:--------->loss: 0.042659\n",
            "Epoch 198:--------->loss: 0.041783\n",
            "Epoch 199:--------->loss: 0.040924\n",
            "Epoch 200:--------->loss: 0.040084\n",
            "Epoch 201:--------->loss: 0.039260\n",
            "Epoch 202:--------->loss: 0.038454\n",
            "Epoch 203:--------->loss: 0.037664\n",
            "Epoch 204:--------->loss: 0.036890\n",
            "Epoch 205:--------->loss: 0.036133\n",
            "Epoch 206:--------->loss: 0.035391\n",
            "Epoch 207:--------->loss: 0.034664\n",
            "Epoch 208:--------->loss: 0.033952\n",
            "Epoch 209:--------->loss: 0.033254\n",
            "Epoch 210:--------->loss: 0.032571\n",
            "Epoch 211:--------->loss: 0.031902\n",
            "Epoch 212:--------->loss: 0.031247\n",
            "Epoch 213:--------->loss: 0.030605\n",
            "Epoch 214:--------->loss: 0.029976\n",
            "Epoch 215:--------->loss: 0.029361\n",
            "Epoch 216:--------->loss: 0.028758\n",
            "Epoch 217:--------->loss: 0.028167\n",
            "Epoch 218:--------->loss: 0.027588\n",
            "Epoch 219:--------->loss: 0.027022\n",
            "Epoch 220:--------->loss: 0.026467\n",
            "Epoch 221:--------->loss: 0.025923\n",
            "Epoch 222:--------->loss: 0.025390\n",
            "Epoch 223:--------->loss: 0.024869\n",
            "Epoch 224:--------->loss: 0.024358\n",
            "Epoch 225:--------->loss: 0.023858\n",
            "Epoch 226:--------->loss: 0.023368\n",
            "Epoch 227:--------->loss: 0.022888\n",
            "Epoch 228:--------->loss: 0.022418\n",
            "Epoch 229:--------->loss: 0.021957\n",
            "Epoch 230:--------->loss: 0.021506\n",
            "Epoch 231:--------->loss: 0.021064\n",
            "Epoch 232:--------->loss: 0.020632\n",
            "Epoch 233:--------->loss: 0.020208\n",
            "Epoch 234:--------->loss: 0.019793\n",
            "Epoch 235:--------->loss: 0.019386\n",
            "Epoch 236:--------->loss: 0.018988\n",
            "Epoch 237:--------->loss: 0.018598\n",
            "Epoch 238:--------->loss: 0.018216\n",
            "Epoch 239:--------->loss: 0.017842\n",
            "Epoch 240:--------->loss: 0.017475\n",
            "Epoch 241:--------->loss: 0.017116\n",
            "Epoch 242:--------->loss: 0.016765\n",
            "Epoch 243:--------->loss: 0.016421\n",
            "Epoch 244:--------->loss: 0.016083\n",
            "Epoch 245:--------->loss: 0.015753\n",
            "Epoch 246:--------->loss: 0.015429\n",
            "Epoch 247:--------->loss: 0.015112\n",
            "Epoch 248:--------->loss: 0.014802\n",
            "Epoch 249:--------->loss: 0.014498\n",
            "Epoch 250:--------->loss: 0.014200\n",
            "Epoch 251:--------->loss: 0.013908\n",
            "Epoch 252:--------->loss: 0.013623\n",
            "Epoch 253:--------->loss: 0.013343\n",
            "Epoch 254:--------->loss: 0.013069\n",
            "Epoch 255:--------->loss: 0.012800\n",
            "Epoch 256:--------->loss: 0.012537\n",
            "Epoch 257:--------->loss: 0.012280\n",
            "Epoch 258:--------->loss: 0.012028\n",
            "Epoch 259:--------->loss: 0.011781\n",
            "Epoch 260:--------->loss: 0.011539\n",
            "Epoch 261:--------->loss: 0.011302\n",
            "Epoch 262:--------->loss: 0.011070\n",
            "Epoch 263:--------->loss: 0.010842\n",
            "Epoch 264:--------->loss: 0.010619\n",
            "Epoch 265:--------->loss: 0.010401\n",
            "Epoch 266:--------->loss: 0.010188\n",
            "Epoch 267:--------->loss: 0.009978\n",
            "Epoch 268:--------->loss: 0.009773\n",
            "Epoch 269:--------->loss: 0.009573\n",
            "Epoch 270:--------->loss: 0.009376\n",
            "Epoch 271:--------->loss: 0.009183\n",
            "Epoch 272:--------->loss: 0.008995\n",
            "Epoch 273:--------->loss: 0.008810\n",
            "Epoch 274:--------->loss: 0.008629\n",
            "Epoch 275:--------->loss: 0.008452\n",
            "Epoch 276:--------->loss: 0.008278\n",
            "Epoch 277:--------->loss: 0.008108\n",
            "Epoch 278:--------->loss: 0.007942\n",
            "Epoch 279:--------->loss: 0.007779\n",
            "Epoch 280:--------->loss: 0.007619\n",
            "Epoch 281:--------->loss: 0.007462\n",
            "Epoch 282:--------->loss: 0.007309\n",
            "Epoch 283:--------->loss: 0.007159\n",
            "Epoch 284:--------->loss: 0.007012\n",
            "Epoch 285:--------->loss: 0.006868\n",
            "Epoch 286:--------->loss: 0.006727\n",
            "Epoch 287:--------->loss: 0.006589\n",
            "Epoch 288:--------->loss: 0.006453\n",
            "Epoch 289:--------->loss: 0.006321\n",
            "Epoch 290:--------->loss: 0.006191\n",
            "Epoch 291:--------->loss: 0.006064\n",
            "Epoch 292:--------->loss: 0.005939\n",
            "Epoch 293:--------->loss: 0.005817\n",
            "Epoch 294:--------->loss: 0.005698\n",
            "Epoch 295:--------->loss: 0.005581\n",
            "Epoch 296:--------->loss: 0.005466\n",
            "Epoch 297:--------->loss: 0.005354\n",
            "Epoch 298:--------->loss: 0.005244\n",
            "Epoch 299:--------->loss: 0.005136\n",
            "Epoch 300:--------->loss: 0.005031\n",
            "Epoch 301:--------->loss: 0.004927\n",
            "Epoch 302:--------->loss: 0.004826\n",
            "Epoch 303:--------->loss: 0.004727\n",
            "Epoch 304:--------->loss: 0.004630\n",
            "Epoch 305:--------->loss: 0.004535\n",
            "Epoch 306:--------->loss: 0.004442\n",
            "Epoch 307:--------->loss: 0.004350\n",
            "Epoch 308:--------->loss: 0.004261\n",
            "Epoch 309:--------->loss: 0.004173\n",
            "Epoch 310:--------->loss: 0.004088\n",
            "Epoch 311:--------->loss: 0.004004\n",
            "Epoch 312:--------->loss: 0.003921\n",
            "Epoch 313:--------->loss: 0.003841\n",
            "Epoch 314:--------->loss: 0.003762\n",
            "Epoch 315:--------->loss: 0.003685\n",
            "Epoch 316:--------->loss: 0.003609\n",
            "Epoch 317:--------->loss: 0.003535\n",
            "Epoch 318:--------->loss: 0.003462\n",
            "Epoch 319:--------->loss: 0.003391\n",
            "Epoch 320:--------->loss: 0.003322\n",
            "Epoch 321:--------->loss: 0.003253\n",
            "Epoch 322:--------->loss: 0.003187\n",
            "Epoch 323:--------->loss: 0.003121\n",
            "Epoch 324:--------->loss: 0.003057\n",
            "Epoch 325:--------->loss: 0.002994\n",
            "Epoch 326:--------->loss: 0.002933\n",
            "Epoch 327:--------->loss: 0.002872\n",
            "Epoch 328:--------->loss: 0.002813\n",
            "Epoch 329:--------->loss: 0.002756\n",
            "Epoch 330:--------->loss: 0.002699\n",
            "Epoch 331:--------->loss: 0.002644\n",
            "Epoch 332:--------->loss: 0.002589\n",
            "Epoch 333:--------->loss: 0.002536\n",
            "Epoch 334:--------->loss: 0.002484\n",
            "Epoch 335:--------->loss: 0.002433\n",
            "Epoch 336:--------->loss: 0.002383\n",
            "Epoch 337:--------->loss: 0.002334\n",
            "Epoch 338:--------->loss: 0.002286\n",
            "Epoch 339:--------->loss: 0.002239\n",
            "Epoch 340:--------->loss: 0.002193\n",
            "Epoch 341:--------->loss: 0.002148\n",
            "Epoch 342:--------->loss: 0.002104\n",
            "Epoch 343:--------->loss: 0.002061\n",
            "Epoch 344:--------->loss: 0.002018\n",
            "Epoch 345:--------->loss: 0.001977\n",
            "Epoch 346:--------->loss: 0.001936\n",
            "Epoch 347:--------->loss: 0.001897\n",
            "Epoch 348:--------->loss: 0.001858\n",
            "Epoch 349:--------->loss: 0.001819\n",
            "Epoch 350:--------->loss: 0.001782\n",
            "Epoch 351:--------->loss: 0.001746\n",
            "Epoch 352:--------->loss: 0.001710\n",
            "Epoch 353:--------->loss: 0.001675\n",
            "Epoch 354:--------->loss: 0.001640\n",
            "Epoch 355:--------->loss: 0.001606\n",
            "Epoch 356:--------->loss: 0.001573\n",
            "Epoch 357:--------->loss: 0.001541\n",
            "Epoch 358:--------->loss: 0.001509\n",
            "Epoch 359:--------->loss: 0.001478\n",
            "Epoch 360:--------->loss: 0.001448\n",
            "Epoch 361:--------->loss: 0.001418\n",
            "Epoch 362:--------->loss: 0.001389\n",
            "Epoch 363:--------->loss: 0.001361\n",
            "Epoch 364:--------->loss: 0.001333\n",
            "Epoch 365:--------->loss: 0.001305\n",
            "Epoch 366:--------->loss: 0.001279\n",
            "Epoch 367:--------->loss: 0.001252\n",
            "Epoch 368:--------->loss: 0.001227\n",
            "Epoch 369:--------->loss: 0.001201\n",
            "Epoch 370:--------->loss: 0.001177\n",
            "Epoch 371:--------->loss: 0.001153\n",
            "Epoch 372:--------->loss: 0.001129\n",
            "Epoch 373:--------->loss: 0.001106\n",
            "Epoch 374:--------->loss: 0.001083\n",
            "Epoch 375:--------->loss: 0.001061\n",
            "Epoch 376:--------->loss: 0.001039\n",
            "Epoch 377:--------->loss: 0.001018\n",
            "Epoch 378:--------->loss: 0.000997\n",
            "Epoch 379:--------->loss: 0.000976\n",
            "Epoch 380:--------->loss: 0.000956\n",
            "Epoch 381:--------->loss: 0.000937\n",
            "Epoch 382:--------->loss: 0.000917\n",
            "Epoch 383:--------->loss: 0.000898\n",
            "Epoch 384:--------->loss: 0.000880\n",
            "Epoch 385:--------->loss: 0.000862\n",
            "Epoch 386:--------->loss: 0.000844\n",
            "Epoch 387:--------->loss: 0.000827\n",
            "Epoch 388:--------->loss: 0.000810\n",
            "Epoch 389:--------->loss: 0.000793\n",
            "Epoch 390:--------->loss: 0.000777\n",
            "Epoch 391:--------->loss: 0.000761\n",
            "Epoch 392:--------->loss: 0.000745\n",
            "Epoch 393:--------->loss: 0.000730\n",
            "Epoch 394:--------->loss: 0.000715\n",
            "Epoch 395:--------->loss: 0.000700\n",
            "Epoch 396:--------->loss: 0.000686\n",
            "Epoch 397:--------->loss: 0.000672\n",
            "Epoch 398:--------->loss: 0.000658\n",
            "Epoch 399:--------->loss: 0.000645\n",
            "Epoch 400:--------->loss: 0.000631\n",
            "Epoch 401:--------->loss: 0.000618\n",
            "Epoch 402:--------->loss: 0.000606\n",
            "Epoch 403:--------->loss: 0.000593\n",
            "Epoch 404:--------->loss: 0.000581\n",
            "Epoch 405:--------->loss: 0.000569\n",
            "Epoch 406:--------->loss: 0.000557\n",
            "Epoch 407:--------->loss: 0.000546\n",
            "Epoch 408:--------->loss: 0.000535\n",
            "Epoch 409:--------->loss: 0.000524\n",
            "Epoch 410:--------->loss: 0.000513\n",
            "Epoch 411:--------->loss: 0.000502\n",
            "Epoch 412:--------->loss: 0.000492\n",
            "Epoch 413:--------->loss: 0.000482\n",
            "Epoch 414:--------->loss: 0.000472\n",
            "Epoch 415:--------->loss: 0.000462\n",
            "Epoch 416:--------->loss: 0.000453\n",
            "Epoch 417:--------->loss: 0.000444\n",
            "Epoch 418:--------->loss: 0.000435\n",
            "Epoch 419:--------->loss: 0.000426\n",
            "Epoch 420:--------->loss: 0.000417\n",
            "Epoch 421:--------->loss: 0.000408\n",
            "Epoch 422:--------->loss: 0.000400\n",
            "Epoch 423:--------->loss: 0.000392\n",
            "Epoch 424:--------->loss: 0.000384\n",
            "Epoch 425:--------->loss: 0.000376\n",
            "Epoch 426:--------->loss: 0.000368\n",
            "Epoch 427:--------->loss: 0.000360\n",
            "Epoch 428:--------->loss: 0.000353\n",
            "Epoch 429:--------->loss: 0.000346\n",
            "Epoch 430:--------->loss: 0.000339\n",
            "Epoch 431:--------->loss: 0.000332\n",
            "Epoch 432:--------->loss: 0.000325\n",
            "Epoch 433:--------->loss: 0.000318\n",
            "Epoch 434:--------->loss: 0.000312\n",
            "Epoch 435:--------->loss: 0.000305\n",
            "Epoch 436:--------->loss: 0.000299\n",
            "Epoch 437:--------->loss: 0.000293\n",
            "Epoch 438:--------->loss: 0.000287\n",
            "Epoch 439:--------->loss: 0.000281\n",
            "Epoch 440:--------->loss: 0.000275\n",
            "Epoch 441:--------->loss: 0.000270\n",
            "Epoch 442:--------->loss: 0.000264\n",
            "Epoch 443:--------->loss: 0.000259\n",
            "Epoch 444:--------->loss: 0.000253\n",
            "Epoch 445:--------->loss: 0.000248\n",
            "Epoch 446:--------->loss: 0.000243\n",
            "Epoch 447:--------->loss: 0.000238\n",
            "Epoch 448:--------->loss: 0.000233\n",
            "Epoch 449:--------->loss: 0.000228\n",
            "Epoch 450:--------->loss: 0.000224\n",
            "Epoch 451:--------->loss: 0.000219\n",
            "Epoch 452:--------->loss: 0.000215\n",
            "Epoch 453:--------->loss: 0.000210\n",
            "Epoch 454:--------->loss: 0.000206\n",
            "Epoch 455:--------->loss: 0.000202\n",
            "Epoch 456:--------->loss: 0.000197\n",
            "Epoch 457:--------->loss: 0.000193\n",
            "Epoch 458:--------->loss: 0.000189\n",
            "Epoch 459:--------->loss: 0.000186\n",
            "Epoch 460:--------->loss: 0.000182\n",
            "Epoch 461:--------->loss: 0.000178\n",
            "Epoch 462:--------->loss: 0.000174\n",
            "Epoch 463:--------->loss: 0.000171\n",
            "Epoch 464:--------->loss: 0.000167\n",
            "Epoch 465:--------->loss: 0.000164\n",
            "Epoch 466:--------->loss: 0.000160\n",
            "Epoch 467:--------->loss: 0.000157\n",
            "Epoch 468:--------->loss: 0.000154\n",
            "Epoch 469:--------->loss: 0.000151\n",
            "Epoch 470:--------->loss: 0.000148\n",
            "Epoch 471:--------->loss: 0.000145\n",
            "Epoch 472:--------->loss: 0.000142\n",
            "Epoch 473:--------->loss: 0.000139\n",
            "Epoch 474:--------->loss: 0.000136\n",
            "Epoch 475:--------->loss: 0.000133\n",
            "Epoch 476:--------->loss: 0.000130\n",
            "Epoch 477:--------->loss: 0.000128\n",
            "Epoch 478:--------->loss: 0.000125\n",
            "Epoch 479:--------->loss: 0.000123\n",
            "Epoch 480:--------->loss: 0.000120\n",
            "Epoch 481:--------->loss: 0.000118\n",
            "Epoch 482:--------->loss: 0.000115\n",
            "Epoch 483:--------->loss: 0.000113\n",
            "Epoch 484:--------->loss: 0.000110\n",
            "Epoch 485:--------->loss: 0.000108\n",
            "Epoch 486:--------->loss: 0.000106\n",
            "Epoch 487:--------->loss: 0.000104\n",
            "Epoch 488:--------->loss: 0.000102\n",
            "Epoch 489:--------->loss: 0.000100\n",
            "Epoch 490:--------->loss: 0.000098\n",
            "Epoch 491:--------->loss: 0.000096\n",
            "Epoch 492:--------->loss: 0.000094\n",
            "Epoch 493:--------->loss: 0.000092\n",
            "Epoch 494:--------->loss: 0.000090\n",
            "Epoch 495:--------->loss: 0.000088\n",
            "Epoch 496:--------->loss: 0.000086\n",
            "Epoch 497:--------->loss: 0.000084\n",
            "Epoch 498:--------->loss: 0.000083\n",
            "Epoch 499:--------->loss: 0.000081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossList"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYbIp4_TbTri",
        "outputId": "aace96c9-d517-42dc-b490-839113cb6ba4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16.534990310668945,\n",
              " 13.49936580657959,\n",
              " 11.101009368896484,\n",
              " 9.204231262207031,\n",
              " 7.702274322509766,\n",
              " 6.511140823364258,\n",
              " 5.564743518829346,\n",
              " 4.8110833168029785,\n",
              " 4.209244728088379,\n",
              " 3.7270357608795166,\n",
              " 3.339124917984009,\n",
              " 3.0255775451660156,\n",
              " 2.770707845687866,\n",
              " 2.56217098236084,\n",
              " 2.390251398086548,\n",
              " 2.2473018169403076,\n",
              " 2.127302408218384,\n",
              " 2.0255138874053955,\n",
              " 1.9382039308547974,\n",
              " 1.8624343872070312,\n",
              " 1.7958887815475464,\n",
              " 1.7367428541183472,\n",
              " 1.6835581064224243,\n",
              " 1.6352001428604126,\n",
              " 1.5907734632492065,\n",
              " 1.5495704412460327,\n",
              " 1.511032223701477,\n",
              " 1.474716305732727,\n",
              " 1.4402719736099243,\n",
              " 1.4074207544326782,\n",
              " 1.3759411573410034,\n",
              " 1.3456562757492065,\n",
              " 1.3164244890213013,\n",
              " 1.288132667541504,\n",
              " 1.2606886625289917,\n",
              " 1.2340185642242432,\n",
              " 1.208061695098877,\n",
              " 1.182767629623413,\n",
              " 1.1580954790115356,\n",
              " 1.134010672569275,\n",
              " 1.1104838848114014,\n",
              " 1.0874899625778198,\n",
              " 1.0650078058242798,\n",
              " 1.0430179834365845,\n",
              " 1.0215044021606445,\n",
              " 1.0004515647888184,\n",
              " 0.9798462986946106,\n",
              " 0.959676206111908,\n",
              " 0.9399294853210449,\n",
              " 0.9205958247184753,\n",
              " 0.9016649723052979,\n",
              " 0.8831274509429932,\n",
              " 0.8649744987487793,\n",
              " 0.8471968770027161,\n",
              " 0.8297868371009827,\n",
              " 0.8127362728118896,\n",
              " 0.796036958694458,\n",
              " 0.7796820998191833,\n",
              " 0.7636637687683105,\n",
              " 0.7479752898216248,\n",
              " 0.7326093316078186,\n",
              " 0.7175595760345459,\n",
              " 0.7028193473815918,\n",
              " 0.688382089138031,\n",
              " 0.6742414832115173,\n",
              " 0.6603915095329285,\n",
              " 0.6468263268470764,\n",
              " 0.6335396766662598,\n",
              " 0.620526134967804,\n",
              " 0.6077799201011658,\n",
              " 0.5952955484390259,\n",
              " 0.5830676555633545,\n",
              " 0.5710909366607666,\n",
              " 0.5593603253364563,\n",
              " 0.5478706359863281,\n",
              " 0.5366170406341553,\n",
              " 0.5255945324897766,\n",
              " 0.5147984623908997,\n",
              " 0.5042241811752319,\n",
              " 0.49386703968048096,\n",
              " 0.4837227165699005,\n",
              " 0.47378671169281006,\n",
              " 0.4640549123287201,\n",
              " 0.4545229375362396,\n",
              " 0.44518670439720154,\n",
              " 0.4360423982143402,\n",
              " 0.4270857572555542,\n",
              " 0.4183131754398346,\n",
              " 0.4097207486629486,\n",
              " 0.40130481123924255,\n",
              " 0.3930617570877075,\n",
              " 0.38498806953430176,\n",
              " 0.37708011269569397,\n",
              " 0.36933472752571106,\n",
              " 0.3617483377456665,\n",
              " 0.35431787371635437,\n",
              " 0.34703993797302246,\n",
              " 0.33991149067878723,\n",
              " 0.33292949199676514,\n",
              " 0.3260910212993622,\n",
              " 0.3193928301334381,\n",
              " 0.31283238530158997,\n",
              " 0.30640658736228943,\n",
              " 0.3001127541065216,\n",
              " 0.29394829273223877,\n",
              " 0.28791043162345886,\n",
              " 0.28199657797813416,\n",
              " 0.2762042284011841,\n",
              " 0.2705307900905609,\n",
              " 0.2649739384651184,\n",
              " 0.2595312297344208,\n",
              " 0.2542002499103546,\n",
              " 0.2489788681268692,\n",
              " 0.24386467039585114,\n",
              " 0.2388555407524109,\n",
              " 0.23394934833049774,\n",
              " 0.22914381325244904,\n",
              " 0.22443705797195435,\n",
              " 0.21982701122760773,\n",
              " 0.21531160175800323,\n",
              " 0.21088898181915283,\n",
              " 0.20655721426010132,\n",
              " 0.20231439173221588,\n",
              " 0.19815869629383087,\n",
              " 0.1940883994102478,\n",
              " 0.19010166823863983,\n",
              " 0.18619687855243683,\n",
              " 0.18237222731113434,\n",
              " 0.17862622439861298,\n",
              " 0.17495717108249664,\n",
              " 0.17136341333389282,\n",
              " 0.16784356534481049,\n",
              " 0.16439592838287354,\n",
              " 0.16101913154125214,\n",
              " 0.15771172940731049,\n",
              " 0.1544722318649292,\n",
              " 0.15129923820495605,\n",
              " 0.14819146692752838,\n",
              " 0.14514750242233276,\n",
              " 0.1421661525964737,\n",
              " 0.13924597203731537,\n",
              " 0.1363857239484787,\n",
              " 0.13358433544635773,\n",
              " 0.13084043562412262,\n",
              " 0.12815286219120026,\n",
              " 0.1255205273628235,\n",
              " 0.12294229120016098,\n",
              " 0.12041694670915604,\n",
              " 0.11794351786375046,\n",
              " 0.11552087217569351,\n",
              " 0.11314801126718521,\n",
              " 0.11082389205694199,\n",
              " 0.10854750126600266,\n",
              " 0.10631787031888962,\n",
              " 0.10413402318954468,\n",
              " 0.10199511051177979,\n",
              " 0.09990003705024719,\n",
              " 0.09784803539514542,\n",
              " 0.09583818912506104,\n",
              " 0.09386958926916122,\n",
              " 0.09194143861532211,\n",
              " 0.09005291014909744,\n",
              " 0.08820313960313797,\n",
              " 0.08639142662286758,\n",
              " 0.08461689203977585,\n",
              " 0.08287878334522247,\n",
              " 0.08117640763521194,\n",
              " 0.07950901240110397,\n",
              " 0.07787588983774185,\n",
              " 0.07627623528242111,\n",
              " 0.07470947504043579,\n",
              " 0.07317488640546799,\n",
              " 0.07167185842990875,\n",
              " 0.07019967585802078,\n",
              " 0.06875773519277573,\n",
              " 0.06734537333250046,\n",
              " 0.06596206873655319,\n",
              " 0.064607173204422,\n",
              " 0.06328008323907852,\n",
              " 0.061980318278074265,\n",
              " 0.06070719659328461,\n",
              " 0.05946020409464836,\n",
              " 0.05823884904384613,\n",
              " 0.057042572647333145,\n",
              " 0.05587093159556389,\n",
              " 0.05472326651215553,\n",
              " 0.05359920486807823,\n",
              " 0.05249826982617378,\n",
              " 0.051419928669929504,\n",
              " 0.05036373436450958,\n",
              " 0.04932921752333641,\n",
              " 0.048315975815057755,\n",
              " 0.0473235547542572,\n",
              " 0.04635147750377655,\n",
              " 0.04539937898516655,\n",
              " 0.04446687176823616,\n",
              " 0.043553490191698074,\n",
              " 0.04265888035297394,\n",
              " 0.041782643646001816,\n",
              " 0.040924426168203354,\n",
              " 0.04008376970887184,\n",
              " 0.039260461926460266,\n",
              " 0.03845399245619774,\n",
              " 0.03766412287950516,\n",
              " 0.03689049184322357,\n",
              " 0.03613274171948433,\n",
              " 0.035390544682741165,\n",
              " 0.03466363251209259,\n",
              " 0.03395160660147667,\n",
              " 0.0332542322576046,\n",
              " 0.03257116302847862,\n",
              " 0.031902141869068146,\n",
              " 0.031246835365891457,\n",
              " 0.030605001375079155,\n",
              " 0.0299763735383749,\n",
              " 0.029360637068748474,\n",
              " 0.02875754050910473,\n",
              " 0.028166817501187325,\n",
              " 0.027588261291384697,\n",
              " 0.02702159620821476,\n",
              " 0.02646654285490513,\n",
              " 0.02592291682958603,\n",
              " 0.02539042942225933,\n",
              " 0.02486889623105526,\n",
              " 0.02435808815062046,\n",
              " 0.02385774813592434,\n",
              " 0.023367704823613167,\n",
              " 0.02288772724568844,\n",
              " 0.022417595610022545,\n",
              " 0.021957123652100563,\n",
              " 0.021506130695343018,\n",
              " 0.02106437087059021,\n",
              " 0.02063172124326229,\n",
              " 0.02020791918039322,\n",
              " 0.01979285664856434,\n",
              " 0.019386284053325653,\n",
              " 0.01898808777332306,\n",
              " 0.018598070368170738,\n",
              " 0.018216034397482872,\n",
              " 0.01784188486635685,\n",
              " 0.017475394532084465,\n",
              " 0.017116442322731018,\n",
              " 0.01676485687494278,\n",
              " 0.016420500352978706,\n",
              " 0.016083212569355965,\n",
              " 0.015752842649817467,\n",
              " 0.015429292805492878,\n",
              " 0.015112369321286678,\n",
              " 0.014801934361457825,\n",
              " 0.014497905969619751,\n",
              " 0.014200103469192982,\n",
              " 0.013908418826758862,\n",
              " 0.013622738420963287,\n",
              " 0.013342905789613724,\n",
              " 0.01306885201483965,\n",
              " 0.012800391763448715,\n",
              " 0.012537471950054169,\n",
              " 0.012279941700398922,\n",
              " 0.012027710676193237,\n",
              " 0.011780652217566967,\n",
              " 0.011538665741682053,\n",
              " 0.01130166556686163,\n",
              " 0.01106951292604208,\n",
              " 0.010842143557965755,\n",
              " 0.010619443841278553,\n",
              " 0.010401315987110138,\n",
              " 0.010187665931880474,\n",
              " 0.009978405199944973,\n",
              " 0.009773435071110725,\n",
              " 0.00957267265766859,\n",
              " 0.009376044385135174,\n",
              " 0.009183465503156185,\n",
              " 0.008994822390377522,\n",
              " 0.00881007220596075,\n",
              " 0.008629096671938896,\n",
              " 0.008451862260699272,\n",
              " 0.008278253488242626,\n",
              " 0.008108201436698437,\n",
              " 0.00794165302067995,\n",
              " 0.00777852488681674,\n",
              " 0.007618763018399477,\n",
              " 0.007462260778993368,\n",
              " 0.007308994885534048,\n",
              " 0.007158847991377115,\n",
              " 0.007011815439909697,\n",
              " 0.006867788266390562,\n",
              " 0.006726712454110384,\n",
              " 0.006588554009795189,\n",
              " 0.0064532142132520676,\n",
              " 0.006320660933852196,\n",
              " 0.00619082897901535,\n",
              " 0.0060636792331933975,\n",
              " 0.005939125549048185,\n",
              " 0.00581713579595089,\n",
              " 0.005697644781321287,\n",
              " 0.005580615717917681,\n",
              " 0.005465987604111433,\n",
              " 0.005353702697902918,\n",
              " 0.005243740510195494,\n",
              " 0.005136024206876755,\n",
              " 0.005030535627156496,\n",
              " 0.004927197005599737,\n",
              " 0.004825986456125975,\n",
              " 0.0047268555499613285,\n",
              " 0.004629770293831825,\n",
              " 0.004534667357802391,\n",
              " 0.004441521596163511,\n",
              " 0.0043502929620444775,\n",
              " 0.00426093116402626,\n",
              " 0.004173404071480036,\n",
              " 0.004087683744728565,\n",
              " 0.00400371290743351,\n",
              " 0.0039214869029819965,\n",
              " 0.0038409288972616196,\n",
              " 0.0037620372604578733,\n",
              " 0.003684762865304947,\n",
              " 0.003609073581174016,\n",
              " 0.003534948453307152,\n",
              " 0.0034623397514224052,\n",
              " 0.003391215344890952,\n",
              " 0.0033215561416000128,\n",
              " 0.0032533376943320036,\n",
              " 0.003186507849022746,\n",
              " 0.0031210610177367926,\n",
              " 0.0030569497030228376,\n",
              " 0.0029941617976874113,\n",
              " 0.0029326535295695066,\n",
              " 0.0028724127914756536,\n",
              " 0.0028134118765592575,\n",
              " 0.002755617955699563,\n",
              " 0.0026990172918885946,\n",
              " 0.0026435840409249067,\n",
              " 0.002589276060461998,\n",
              " 0.002536090323701501,\n",
              " 0.0024840026162564754,\n",
              " 0.0024329754523932934,\n",
              " 0.002382998587563634,\n",
              " 0.002334058051928878,\n",
              " 0.0022861105389893055,\n",
              " 0.0022391597740352154,\n",
              " 0.002193162450566888,\n",
              " 0.002148109721019864,\n",
              " 0.0021039878483861685,\n",
              " 0.002060773316770792,\n",
              " 0.0020184433087706566,\n",
              " 0.0019769847858697176,\n",
              " 0.001936376211233437,\n",
              " 0.0018966024508699775,\n",
              " 0.0018576435977593064,\n",
              " 0.0018194812582805753,\n",
              " 0.0017821128712967038,\n",
              " 0.0017455025808885694,\n",
              " 0.001709653064608574,\n",
              " 0.001674532308243215,\n",
              " 0.0016401390312239528,\n",
              " 0.0016064424999058247,\n",
              " 0.0015734500484541059,\n",
              " 0.001541123609058559,\n",
              " 0.0015094674890860915,\n",
              " 0.001478463993407786,\n",
              " 0.0014480958925560117,\n",
              " 0.0014183478197082877,\n",
              " 0.00138921441975981,\n",
              " 0.0013606831198558211,\n",
              " 0.0013327341293916106,\n",
              " 0.001305354293435812,\n",
              " 0.0012785439612343907,\n",
              " 0.001252282876521349,\n",
              " 0.0012265631230548024,\n",
              " 0.0012013665400445461,\n",
              " 0.0011766934767365456,\n",
              " 0.0011525246081873775,\n",
              " 0.001128848409280181,\n",
              " 0.001105663599446416,\n",
              " 0.0010829492239281535,\n",
              " 0.0010607092408463359,\n",
              " 0.001038919435814023,\n",
              " 0.0010175801580771804,\n",
              " 0.000996676622889936,\n",
              " 0.0009762049303390086,\n",
              " 0.0009561547194607556,\n",
              " 0.0009365174337290227,\n",
              " 0.0009172820136882365,\n",
              " 0.0008984383312053978,\n",
              " 0.0008799834758974612,\n",
              " 0.0008619095315225422,\n",
              " 0.0008442013640888035,\n",
              " 0.0008268618839792907,\n",
              " 0.0008098799153231084,\n",
              " 0.000793243816588074,\n",
              " 0.0007769528892822564,\n",
              " 0.0007609911845065653,\n",
              " 0.0007453595753759146,\n",
              " 0.0007300532888621092,\n",
              " 0.0007150571327656507,\n",
              " 0.0007003709324635565,\n",
              " 0.0006859840359538794,\n",
              " 0.0006718916702084243,\n",
              " 0.0006580916815437376,\n",
              " 0.0006445747567340732,\n",
              " 0.0006313352496363223,\n",
              " 0.0006183634395711124,\n",
              " 0.0006056631682440639,\n",
              " 0.0005932235508225858,\n",
              " 0.0005810409202240407,\n",
              " 0.0005691035185009241,\n",
              " 0.0005574168753810227,\n",
              " 0.0005459651001729071,\n",
              " 0.0005347519763745368,\n",
              " 0.0005237652221694589,\n",
              " 0.0005130068748258054,\n",
              " 0.0005024666315875947,\n",
              " 0.000492146413307637,\n",
              " 0.0004820370813831687,\n",
              " 0.00047213854850269854,\n",
              " 0.00046244150144048035,\n",
              " 0.0004529404977802187,\n",
              " 0.00044363769120536745,\n",
              " 0.00043452359386719763,\n",
              " 0.0004255990497767925,\n",
              " 0.0004168572777416557,\n",
              " 0.00040829426143318415,\n",
              " 0.00039990790537558496,\n",
              " 0.00039169317460618913,\n",
              " 0.0003836491669062525,\n",
              " 0.0003757673257496208,\n",
              " 0.0003680506197270006,\n",
              " 0.0003604887460824102,\n",
              " 0.0003530846443027258,\n",
              " 0.0003458303108345717,\n",
              " 0.00033872792846523225,\n",
              " 0.00033176757278852165,\n",
              " 0.00032495547202415764,\n",
              " 0.0003182801010552794,\n",
              " 0.000311742682242766,\n",
              " 0.000305340246995911,\n",
              " 0.0002990679058711976,\n",
              " 0.0002929244074039161,\n",
              " 0.00028690777253359556,\n",
              " 0.0002810128789860755,\n",
              " 0.00027524118195287883,\n",
              " 0.00026958732632920146,\n",
              " 0.0002640515158418566,\n",
              " 0.00025862662005238235,\n",
              " 0.00025331403594464064,\n",
              " 0.0002481123956386,\n",
              " 0.0002430148742860183,\n",
              " 0.0002380218356847763,\n",
              " 0.00023313157726079226,\n",
              " 0.0002283432986587286,\n",
              " 0.00022365340555552393,\n",
              " 0.00021906128677073866,\n",
              " 0.0002145615144399926,\n",
              " 0.00021015375386923552,\n",
              " 0.0002058368845609948,\n",
              " 0.00020160898566246033,\n",
              " 0.00019746593898162246,\n",
              " 0.000193409898201935,\n",
              " 0.000189436788787134,\n",
              " 0.00018554563575889915,\n",
              " 0.0001817357842810452,\n",
              " 0.0001780016318662092,\n",
              " 0.0001743462635204196,\n",
              " 0.00017076377116609365,\n",
              " 0.00016725713794585317,\n",
              " 0.00016382372996304184,\n",
              " 0.00016045647498685867,\n",
              " 0.0001571603206684813,\n",
              " 0.00015393266221508384,\n",
              " 0.00015077147691044956,\n",
              " 0.00014767520769964904,\n",
              " 0.00014464139530900866,\n",
              " 0.00014167134941089898,\n",
              " 0.00013876154844183475,\n",
              " 0.000135910275275819,\n",
              " 0.0001331188395852223,\n",
              " 0.00013038479664828628,\n",
              " 0.0001277068367926404,\n",
              " 0.0001250818168045953,\n",
              " 0.00012251395673956722,\n",
              " 0.00011999707930954173,\n",
              " 0.00011753213038900867,\n",
              " 0.00011511889169923961,\n",
              " 0.00011275339056737721,\n",
              " 0.00011043666017940268,\n",
              " 0.0001081684822565876,\n",
              " 0.00010594742343528196,\n",
              " 0.0001037707770592533,\n",
              " 0.00010163852130062878,\n",
              " 9.955069253919646e-05,\n",
              " 9.750734170665964e-05,\n",
              " 9.55040377448313e-05,\n",
              " 9.354178473586217e-05,\n",
              " 9.162125206785277e-05,\n",
              " 8.973918738774955e-05,\n",
              " 8.789565617917106e-05,\n",
              " 8.60906220623292e-05,\n",
              " 8.432180766249076e-05,\n",
              " 8.25903334771283e-05,\n",
              " 8.089328912319615e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample data\n",
        "x = lossList\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(x)\n",
        "\n",
        "# Add labels and a title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Epoch vs Loss')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "dCdPMtkMaWwV",
        "outputId": "f403bcb4-cb38-4e19-e7b7-ff604c07dca1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The resulting mode weights and biases.\n",
        "\n",
        "for cell in modelPytorch.parameters():\n",
        "  print(cell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJV9foGOL8FQ",
        "outputId": "53d9d1bb-738b-4bd2-e668-806cab075fba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[1.9962]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.9882], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test predicting a value not seen in the original training set.\n",
        "\n",
        "modelPytorch(torch.tensor(10.).reshape(-1,1))"
      ],
      "metadata": {
        "id": "zFgZvyi1ezkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177a980c-2906-4b42-ba00-80d5a76325c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18.9738]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "\n",
        "|Pytorch|Keras|\n",
        "|-------|-----|\n",
        "|More coding|Less coding|\n",
        "|Gray box|Black box|\n",
        "|Does not accepts Numpy arrays in model| Does accept Numpy arrays|\n",
        "|Pythonic Loop|Built-in Loop|\n",
        "|Faster learning|Slower Learning|\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "luLsh93hSNgx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_U-zBscc-fo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k15GF-MaMT9P"
      }
    }
  ]
}